{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem statement : A customer should be given loan or not.\n",
    "# Target : Multi-class classification (P1/P2/P3/P4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Respect to bank\n",
    "\n",
    "# Asset = Loan Product\n",
    "\n",
    "# House Loan\n",
    "# Car Loan\n",
    "# Vehicle Loan\n",
    "# Group Loan\n",
    "# Education Loan\n",
    "# Credit Card \n",
    "\n",
    "# What is Asset for bank , that is Liability for Customer.\n",
    "\n",
    "# Liability = Bank loss\n",
    "\n",
    "# Current Account\n",
    "# Savings Account\n",
    "# (CaSa)\n",
    "\n",
    "# Fixed Deposit(Term Deposits)\n",
    "# RD(Recurring Deposit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDPD (0) = NDA (Non delinquint account) =    Timely payment account\\nDPD(0 to 30) = SMA1 (Standard Monitoring Account)\\nDPD(31 to 60) = SMA2 (Standard Monitoring Account)\\nDPD(61 to 90) = SMA3 (Standard Monitoring Account)\\nDPD(90 to 180) = NPA\\nDPD(>180) = Written-off (Loan which is not present), NPA improve = Loan Portfolio quality of bank will be better , Market sentiment will be good, bank share market value will bw good.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NPA \n",
    "\n",
    "# NPA = Non Performing Asset, Npa is less means bank ka loan portfolio,market sentiment accha hai.\n",
    "# (The asset or loan that is defaulted)\n",
    "\n",
    "# 1. Disbursed Amount = Loan amount given to a customer.\n",
    "# 2. OSP = Outstanding Principle\n",
    "\n",
    "# example : 1 lakh loan, 8000 EMI , OSP? 40000 paid , balance = 60000, OSP should be zero at the end of loan cycle.\n",
    "\n",
    "# Amortization\n",
    "\n",
    "# 3. DPD = Days past due\n",
    "#EMI should be paid on 10th apr, but someone paid on 12th apr, DPD = 2 , DPD should be ideally 0 \n",
    "\n",
    "# 4. PAR\n",
    "# Portfolio at risk\n",
    "# OSP when DPD > 0 day\n",
    "\n",
    "# NPA \n",
    "# NPA is a loan account when DPD > 90 days\n",
    "# NPA Account\n",
    "\n",
    "# Credit Risk Types in Banking\n",
    "\n",
    "\"\"\"\n",
    "DPD (0) = NDA (Non delinquint account) =    Timely payment account\n",
    "DPD(0 to 30) = SMA1 (Standard Monitoring Account)\n",
    "DPD(31 to 60) = SMA2 (Standard Monitoring Account)\n",
    "DPD(61 to 90) = SMA3 (Standard Monitoring Account)\n",
    "DPD(90 to 180) = NPA\n",
    "DPD(>180) = Written-off (Loan which is not present), NPA improve = Loan Portfolio quality of bank will be better , Market sentiment will be good, bank share market value will bw good.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NPA hs two parts.\n",
    "# GNPA = Gross NPA (3-5 %) = OSP default\n",
    "# NNPA = Net NPA = (0.01 to 0.06) = Provisioning Amount subtracted\n",
    "# Bank quality assess\n",
    "# We should check GNPA  to think should we invest money on that bank in share market.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit score is a measure to tell how much good someone's lending history is.\n",
    "#  Below 580: poor.\n",
    "# 580 to 669: fair.\n",
    "# 670 to 739: good.\n",
    "# 740 to 799: very good.\n",
    "# 800 and above: exceptional.\n",
    "# Its range is almost 350 to 900 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loan is technicaaly called TL(Trade Line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bank lends money to people who don't need money.\n",
    "# Bank try to avoid giving loan to those who need money.\n",
    "# Credit card utilization is how much of credit card limit customer utilizs. \n",
    "# More credit card utilization means customer is credit hungry,bad at managing finance.\n",
    "# Customer with less credit card utilization is good for bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit card enquiries means for how many times someone has applied ffor credit card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -99999 means null value/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWe want them to be associated , so we will put associated in alternate hypothesis\\n1. H0 : Null hypothesis\\nNot associated\\n\\n2. H1 : Alternate hypothesis\\nAssociated \\n\\n3. Alpha (assumed)\\nSignificance level\\nStrictness level \\n5% = 0.05 generally here (slightly less risky)\\n0.00001 se kuch bhi\\nless risky projects = high alpha\\nmore risky projects = less alpha \\n\\n4. Confidence level\\n= 1 - alpha\\n\\n5. Calculate the evidence against H0\\np-value\\np value is calculated using tests T-Test,Chi-square test,Anova and a concept of Degree of freedom \\n\\n6. \\np value <= alpha\\nReject H0\\n\\np value > alpha\\nFail to reject H0\\n{Not accept H0, fail to reject , because alpha is an assumed value.\\nWe want to get evidence against H0, we are just not able to get evidence in this case.\\nCompared with court of law.}\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hypothesis Testing\n",
    "# Inferential statistics\n",
    "\n",
    "# Are these two associated : marital status and approved flag ?\n",
    "\n",
    "\"\"\"\n",
    "We want them to be associated , so we will put associated in alternate hypothesis\n",
    "1. H0 : Null hypothesis\n",
    "Not associated\n",
    "\n",
    "2. H1 : Alternate hypothesis\n",
    "Associated \n",
    "\n",
    "3. Alpha (assumed)\n",
    "Significance level\n",
    "Strictness level \n",
    "5% = 0.05 generally here (slightly less risky)\n",
    "0.00001 se kuch bhi\n",
    "less risky projects = high alpha\n",
    "more risky projects = less alpha \n",
    "\n",
    "4. Confidence level\n",
    "= 1 - alpha\n",
    "\n",
    "5. Calculate the evidence against H0\n",
    "p-value\n",
    "p value is calculated using tests T-Test,Chi-square test,Anova and a concept of Degree of freedom \n",
    "\n",
    "6. \n",
    "p value <= alpha\n",
    "Reject H0\n",
    "\n",
    "p value > alpha\n",
    "Fail to reject H0\n",
    "{Not accept H0, fail to reject , because alpha is an assumed value.\n",
    "We want to get evidence against H0, we are just not able to get evidence in this case.\n",
    "Compared with court of law.}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are these tests ?\n",
    "# Chisquare = categorical  vs categorical\n",
    "# T test, Anova = cat(2 categories in case of t test , more than 2 category in case of anova) vs num "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multicollinearity vs Correlation\n",
    "# Multicollinearity is Predictibility of eah features by other features.\n",
    "# Correlation is similarity between two features, but it is not used for all funcions, only linear functions.\n",
    "# Multicollinearity is used more for industry projects.\n",
    "# Correlation is specific to linear relationships.For example , in convex function, correlation gives misleading values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For numerical columns, more than 2 categories,, so anova test .\n",
    "# Multicollinearity will help in this case.\n",
    "# VIF (Variation Inflation Factor) is used to identify multicollinearity.\n",
    "# Takes R squared for each feature and eliminate if crosses a threshold.\n",
    "# VIF = 1/(1- R-squared for each feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  VIF ranges from 1 to infinity.\n",
    "# VIF = 1 : No multicollinearity\n",
    "# VIF between 1 and 5 : Low multicollinearity\n",
    "# VIF between 5 and 10 : Moderate Multicollinearity\n",
    "# VIF above 10 : High Multicollinearity\n",
    "# For this case study , we generally take threshold VIF as 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why Columns Are Removed Due to High VIF:\n",
    "# Columns with high VIF values are removed to address multicollinearity. Hereâ€™s why they fail in the context of multicollinearity:\n",
    "\n",
    "# Redundancy:\n",
    "\n",
    "# Columns with high VIF are redundant because they provide little new information beyond what is already explained by other predictors.\n",
    "# Their inclusion does not add value to the model and complicates the interpretation of the results.\n",
    "# Instability:\n",
    "\n",
    "# These columns cause the model coefficients to be unstable, leading to unreliable and non-reproducible results.\n",
    "# The coefficients become sensitive to small changes in the model, making the model's behavior unpredictable.\n",
    "# Inflated Standard Errors:\n",
    "\n",
    "# High VIF values indicate that the standard errors of the corresponding predictors are inflated.\n",
    "# This inflation reduces the statistical significance of these predictors, making it harder to detect their true effect on the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel vs Sequential VIF\n",
    "# Sequential VIF is better way or correct way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi square test\n",
    "# categorical ko categorical ke saath compare karenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "# Accuracy is sum of true positives and true negatives divided by sum of all(true positive,true negative,false positive,false negative).\n",
    "# Recall is for each class, true positives or negatives divided by 100.\n",
    "# Accuracy is out of total values,how many are correctly predicted.\n",
    "# Recall is out of total values for a class,how many are correctly predicted.\n",
    "# Precision is out of my total predicted values for a class,how many are correctly predicted.\n",
    "# F1 Score is 2*P*R/(P+R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we measure accuracy,we need test accuracy, y_pred on x_test versus y_test.\n",
    "# We we need to check underfitting,we need train accuracy, y_pred on x_train versus y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlcourse301",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
